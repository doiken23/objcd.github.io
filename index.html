<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Detecting Object-Level Scene Changes in Images with Viewpoint Differences Using Graph Matching.">
  <meta name="keywords" content="change detection, remote sensing, object detection, graph matching">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Detecting Object-Level Scene Changes in Images with Viewpoint Differences Using Graph Matching</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Detecting Object-Level Scene Changes in Images with Viewpoint Differences Using Graph Matching</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=YdFXrWcAAAAJ">Kento Doi</a><sup>1, 2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.co.jp/citations?user=tsKeE-0AAAAJ">Ryuhei Hamaguchi</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://kensakurada.github.io/">Yusuke Iwasawa</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="http://onishi-lab.jp/">Masaki Onishi</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="http://ymatsuo.com/">Yutaka Matsuo</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://kensakurada.github.io/">Ken Sakurada</a><sup>2</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <p>
              <span class="author-block"><sup>1</sup>The University of Tokyo</span>
            </p>
            <p>
              <span class="author-block"><sup>2</sup>National Institute of Advanced Industrial Science and Technology (AIST)</span>
            </p>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://www.mdpi.com/2072-4292/14/17/4225"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="content has-text-justified">
        <figure>
          <img
            src="https://www.mdpi.com/remotesensing/remotesensing-14-04225/article_deploy/html/images/remotesensing-14-04225-ag.png"
            width="720">
        </figure>

        <p>
          Our proposed network robustly detects <it>object-level</it> scene changes from an image pair with viewpoint differences.
        </p>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          We developed a robust <i>object-level</i> change detection method
          that could capture distinct scene changes in an image pair with viewpoint differences.
          </p>
          <p>
          To achieve this, we designed a network that could detect object-level changes in an image pair.
          In contrast to previous studies, we considered the change detection task
          as a graph matching problem for two object graphs that were extracted from each image.
          By virtue of this, the proposed network more robustly detected object-level changes
          with viewpoint differences than existing pixel-level approaches.
          In addition, the network did not require pixel-level change annotations,
          which have been required in previous studies.
          Specifically, the proposed network extracted the objects in each image
          using an object detection module and then constructed correspondences
          between the objects using an object matching module.
          Finally, the network detected objects that appeared or disappeared
          in a scene using the correspondences that were obtained between the objects.
          </p>
          <p>
          To verify the effectiveness of the proposed network, we created a synthetic dataset of images that contained object-level changes.
          In experiments on the created dataset, the proposed method improved the F1 score of conventional methods by more than 40%.
          <!-- Our synthetic dataset will be available publicly online. -->
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Object-Level Scene Change Detection</h2>
    <div class="content has-text-justified">
      <p>
        We proposed a novel deep neural network that can detect <it>object-level</it> changes 
        robustly to viewpoint differences between images.
        As shown in the figure below, our proposed network detects object-level changes by
        (1) extracting objects from an image pair using an object detection module and
        (2) matching objects to detect changes using a graph matching module.
        Finally, the proposed network outputs scene changes in bounding box or instance mask format.
      </p>
      <figure>
          <img
            src="https://www.mdpi.com/remotesensing/remotesensing-14-04225/article_deploy/html/images/remotesensing-14-04225-g001.png"
            width="720">
      </figure>
    </div>
  </div>
</section>


<section class="section" id="dataset">
  <div class="container is-max-desktop">
    <h2 class="title is-3">SOCD: Synthesized Object-Level Change Detection Dataset</h2>
    <div class="content has-text-justified">
      <p>
        The SOCD dataset is the first dataset that can be used to evaluate object-level change detection.
        The SOCD dataset comprises 15,000 perspective image pairs and object-level change labels
        synthesized by <a href="https://github.com/carla-simulator/carla">CARLA simulator</a> [1].
      </p>
    </div>

    <h4 class="subtitle is-4">Images</h4>
    <div class="content has-text-justified">
      <p>
        The SOCD dataset contains 15,000 perspective image pairs rendered
        from cameras in city-like environments of the CARLA simulator.
        The field of view of the images is 90-degree, and the image size is 1080 &times 1080.
      </p>
    </div>

    <h4 class="subtitle is-4">Labels</h4>
    <div class="content has-text-justified">
      <p>
        In addition to pixel-level (semantic change mask) and object-level (instance mask) change labels,
        semantic masks for entire scenes, depth images, and correspondences between objects in image pairs
        are available.
      </p>
      <p>
        There are four object categories:
        <ul>
            <li>Buildings,</li>
            <li>Cars,</li>
            <li>Poles,</li>
            <li>traffic signs and lights.</li>
        </ul>
      </p>
      <p>
        The figure below shows examples of the instance mask and bounding box for the objects/changes.
        The light blue is buildings, pink is cars, and green is poles.
        Please see <a href="">this repository</a> if you would like to know more about the label format.
      </p>
      <figure>
        <img
          src="https://www.mdpi.com/remotesensing/remotesensing-14-04225/article_deploy/html/images/remotesensing-14-04225-g005.png"
          width="640">
      </figure>
    </div>

    <h4 class="subtitle is-4">Viewpoint differences</h4>
    <div class="content has-text-justified">
      <p>
        We rendered the image pairs with varying viewpoint differences 
        to investigate the robustness of the change detection method to viewpoint differences.
        Specifically, the SOCD dataset has four categories 
        {S<sub>1</sub>, S<sub>2</sub>, S<sub>3</sub>, S<sub>4</sub>} 
        with varying yaw angle differences.
        In S<sub>1</sub>, there is no difference in yaw angle.
        In S<sub>2, 3, 4</sub>, the difference in yaw angle is uniformly sampled within the ranges of
        [0, 10], [10, 20], and [20, 30].
      </p>
      <p>
        The figure below shows image pairs from the dataset with viewpoint differences.
        For more details, please see <a href="https://www.mdpi.com/2072-4292/14/17/4225#">our paper</a>.
      </p>
      <figure>
        <img
          src="https://www.mdpi.com/remotesensing/remotesensing-14-04225/article_deploy/html/images/remotesensing-14-04225-g007.png"
          width="720">
      </figure>
    </div>

    <h4 class="subtitle is-4">Directory Structure</h4>
    <div class="content has-text-justified">
      <pre><code>root/
├── Town01/         # RGB images
├── Town02/
├── Town03/
├── chmasks/        # binary change masks
│   ├── Town01/
│   ├── Town02/
│   └── Town03/
├── semmask/        # semantic masks
│   ├── Town01/
│   ├── Town02/
│   └── Town03/
└── labels/         # label files
    ├── train[1-4].json
    ├── val[1-4].json
    └── test[1-4].json</code></pre>
    </div>

  </div>
</section>


<section class="section" id="results">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Experimental Results</h2>
    <div class="content has-text-justified">
      <p>
        To demonstrate the effectiveness of our proposed network,
        we conducted experiments using the SOCD dataset.
      </p>
    </div>

    <h4 class="subtitle is-4">Qualitative Results</h4>
    <div class="content has-text-justified">
      <p>
        As shown in the figure below, our network successfully detected object-level changes,
        even when viewpoint differences existed between the images (from the 2nd row to the 6th row).
        The proposed method also successfully detected thin objects (the 1st row) and small objects (the 5th row).
        Furthermore, the change masks were more accurate than those in the existing methods,
        when the scale of the viewpoint difference was relatively large (the 4th, 5th, and 6th rows).
      </p>
      <figure>
        <img
          src="https://www.mdpi.com/remotesensing/remotesensing-14-04225/article_deploy/html/images/remotesensing-14-04225-g008.png"
          width="800">
      </figure>
    </div>

    <h4 class="subtitle is-4">Quantitative Results</h4>
    <div class="content has-text-justified">
      <p>
        The table below presents the results of the object-level change detection methods using the SOCD dataset.
        When there were no viewpoint differences between the images, the baseline method with CSSCDNet [3] produced the best score.
        By contrast, when there were viewpoint differences between the images, 
        the proposed method outperformed all baseline methods and improved the score by more than 40%.
      </p>
      <table>
        <tr>
            <th></th> <th align=center colspan="4">Viewpoint difference</th>
        </tr>
        <tr>
            <th>Method</th> <th align=center>Δyaw=0°</th> <th align=center>Δyaw=0-10°</th> <th align=center>Δyaw=10-20°</th> <th align=center>Δyaw=20-30°</th>
        </tr>
        <tr>
            <td>ChangeNet [2]</td> <td align=center>0.219</td> <td align=center>0.142</td> <td align=center>0.155</td> <td align=center>0.133</td>
        </tr>
        <tr>
            <td>CSSCDNet [3] (w/o correlation layer)</td> <td align=center>0.453</td> <td align=center>0.234</td> <td align=center>0.241</td> <td align=center>0.205</td>
        </tr>
        <tr>
            <td>CSSCDNet [3] (w/ correlation layer)</td> <td align=center>0.508</td> <td align=center>0.274</td> <td align=center>0.258</td> <td align=center>0.208</td>
        </tr>
        <tr>
            <td>Ours (w/o graph matching module)</td> <td align=center>0.406</td> <td align=center>0.337</td> <td align=center>0.335</td> <td align=center>0.295</td>
        </tr>
        <tr>
            <td>Ours</td> <td align=center>0.463</td> <td align=center>0.401</td> <td align=center>0.396</td> <td align=center>0.354</td>
        </tr>
        <tr>
            <td>Ours (w/ GT mask)</td> <td align=center>0.852</td> <td align=center>0.650</td> <td align=center>0.652</td> <td align=center>0.546</td>
        </tr>
      </table>
    </div>
  </div>
</section>


<section class="section" id="copyright&licenses">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">Copyright & License</h2>
      <p>
        The SOCD dataset and sample code on this page are copyrighted
        by <a href="https://www.aist.go.jp/index_en.html">
            the National Institute of Advanced Industrial Science and Technology (AIST)</a> 
        and published under the <a href="https://creativecommons.org/licenses/by/4.0/deed.en">CC BY 4.0</a> license.
      </p>
  </div>
</section>


<section class="section" id="bibtex">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">BibTeX</h2>
    <pre><code>@article{objcd,
  author    = {Doi, Kento and Hamaguchi, Ryuhei and Iwasawa, Yusuke and Onishi, Masaki and Matsuo, Yutaka and Sakurada, Ken},
  title     = {Detecting Object-Level Scene Changes in Images with Viewpoint Differences Using Graph Matching},
  journal   = {Remote Sensing},
  volume    = {14},
  number    = {17},
  year      = {2022},
}</code></pre>
  </div>
</section>


<!-- Acknowledgement -->
<section class="section" id="acknowledgement">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">Acknowledgment</h2>
    <p>
      We wish to thank A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun for developing an excellent simulator.
    </p>
    <p>
      This work was partially supported by JSPS KAKENHI (grantnumber:20H04217).
    </p>
  </div>
</section>


<!-- References -->
<section class="section" id="references">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">References</h2>
    <ol>
      <li> A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, V. Koltun. (2017).
          "CARLA: An Open Urban Driving Simulator.": 
          In Proceedings of the 1st Annual Conference on Robot Learning, 78, 1–16.
          (webpage: <a href="https://carla.org/">https://carla.org/</a>,
          license: <a href="https://github.com/carla-simulator/carla#license">https://github.com/carla-simulator/carla#licenses</a>)
      <li> A. Varghese, J. Gubbi, A. Ramaswamy. (2018).
          "ChangeNet: A Deep Learning Architecture for Visual Change Detection.":
          In Proceedings of the ECCV Workshop.
      <li> K. Sakurada, M. Shibuya, W. Wang. (2020).
          "Weakly Supervised Silhouette-based Semantic Scene Change Detection.":
          In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 6861–6867.
    </ol>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://homes.cs.washington.edu/~kpar/nerfies/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This webpage is built with the template from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
            We sincerely thank <a href="https://keunhong.com/">Keunhong Park</a> for developing and open-sourcing this template.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
