<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Detecting Object-Level Scene Changes in Images with Viewpoint Differences Using Graph Matching.">
  <meta name="keywords" content="change detection, remote sensing, object detection, graph matching">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Detecting Object-Level Scene Changes in Images with Viewpoint Differences Using Graph Matching</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Detecting Object-Level Scene Changes in Images with Viewpoint Differences Using Graph Matching</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=YdFXrWcAAAAJ">Kento Doi</a><sup>1, 2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.co.jp/citations?user=tsKeE-0AAAAJ">Ryuhei Hamaguchi</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://kensakurada.github.io/">Yusuke Iwasawa</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="http://onishi-lab.jp/">Masaki Onishi</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="http://ymatsuo.com/">Yutaka Matsuo</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://kensakurada.github.io/">Ken Sakurada</a><sup>2</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <p>
              <span class="author-block"><sup>1</sup>The University of Tokyo</span>
            </p>
            <p>
              <span class="author-block"><sup>2</sup>National Institute of Advanced Industrial Science and Technology (AIST)</span>
            </p>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://www.mdpi.com/2072-4292/14/17/4225"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="./dataset.html"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="content has-text-justified">
        <figure>
          <img
            src="https://www.mdpi.com/remotesensing/remotesensing-14-04225/article_deploy/html/images/remotesensing-14-04225-ag.png"
            width="720">
        </figure>

        <p>
          Our proposed network robustly detects <it>object-level</it> scene changes from an image pair with viewpoint differences.
        </p>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          We developed a robust <i>object-level</i> change detection method
          that could capture distinct scene changes in an image pair with viewpoint differences.
          </p>
          <p>
          To achieve this, we designed a network that could detect object-level changes in an image pair.
          In contrast to previous studies, we considered the change detection task
          as a graph matching problem for two object graphs that were extracted from each image.
          By virtue of this, the proposed network more robustly detected object-level changes
          with viewpoint differences than existing pixel-level approaches.
          In addition, the network did not require pixel-level change annotations,
          which have been required in previous studies.
          Specifically, the proposed network extracted the objects in each image
          using an object detection module and then constructed correspondences
          between the objects using an object matching module.
          Finally, the network detected objects that appeared or disappeared
          in a scene using the correspondences that were obtained between the objects.
          </p>
          <p>
          To verify the effectiveness of the proposed network, we created a synthetic dataset of images that contained object-level changes.
          In experiments on the created dataset, the proposed method improved the F1 score of conventional methods by more than 40%.
          <!-- Our synthetic dataset will be available publicly online. -->
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Object-Level Scene Change Detection</h2>
    <div class="content has-text-justified">
      <p>
        We proposed a novel deep neural network that can detect <it>object-level</it> changes 
        robustly to viewpoint differences between images.
        As shown in the figure below, our proposed network detects object-level changes by
        (1) extracting objects from an image pair using an object detection module and
        (2) matching objects to detect changes using a graph matching module.
        Finally, the proposed network outputs scene changes in bounding box or instance mask format.
      </p>
      <figure>
          <img
            src="https://www.mdpi.com/remotesensing/remotesensing-14-04225/article_deploy/html/images/remotesensing-14-04225-g001.png"
            width="720">
      </figure>
    </div>
  </div>
</section>


<section class="section" id="results">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Experimental Results</h2>
    <div class="content has-text-justified">
      <p>
        To demonstrate the effectiveness of our proposed network,
        we conducted experiments using the <a href="./dataset.html">SOCD dataset</a>.
      </p>
    </div>

    <h4 class="subtitle is-4">Qualitative Results</h4>
    <div class="content has-text-justified">
      <p>
        As shown in the figure below, our network successfully detected object-level changes,
        even when viewpoint differences existed between the images (from the 2nd row to the 6th row).
        The proposed method also successfully detected thin objects (the 1st row) and small objects (the 5th row).
        Furthermore, the change masks were more accurate than those in the existing methods,
        when the scale of the viewpoint difference was relatively large (the 4th, 5th, and 6th rows).
      </p>
      <figure>
        <img
          src="https://www.mdpi.com/remotesensing/remotesensing-14-04225/article_deploy/html/images/remotesensing-14-04225-g008.png"
          width="800">
      </figure>
    </div>

    <h4 class="subtitle is-4">Quantitative Results</h4>
    <div class="content has-text-justified">
      <p>
        The table below presents the results of the object-level change detection methods using the SOCD dataset.
        When there were no viewpoint differences between the images, the baseline method with CSSCDNet [3] produced the best score.
        By contrast, when there were viewpoint differences between the images, 
        the proposed method outperformed all baseline methods and improved the score by more than 40%.
      </p>
      <table>
        <tr>
            <th></th> <th align=center colspan="4">Viewpoint difference</th>
        </tr>
        <tr>
            <th>Method</th> <th align=center>Δyaw=0°</th> <th align=center>Δyaw=0-10°</th> <th align=center>Δyaw=10-20°</th> <th align=center>Δyaw=20-30°</th>
        </tr>
        <tr>
            <td>ChangeNet [1]</td> <td align=center>0.219</td> <td align=center>0.142</td> <td align=center>0.155</td> <td align=center>0.133</td>
        </tr>
        <tr>
            <td>CSSCDNet [2] (w/o correlation layer)</td> <td align=center>0.453</td> <td align=center>0.234</td> <td align=center>0.241</td> <td align=center>0.205</td>
        </tr>
        <tr>
            <td>CSSCDNet [2] (w/ correlation layer)</td> <td align=center>0.508</td> <td align=center>0.274</td> <td align=center>0.258</td> <td align=center>0.208</td>
        </tr>
        <tr>
            <td>Ours (w/o graph matching module)</td> <td align=center>0.406</td> <td align=center>0.337</td> <td align=center>0.335</td> <td align=center>0.295</td>
        </tr>
        <tr>
            <td>Ours</td> <td align=center>0.463</td> <td align=center>0.401</td> <td align=center>0.396</td> <td align=center>0.354</td>
        </tr>
        <tr>
            <td>Ours (w/ GT mask)</td> <td align=center>0.852</td> <td align=center>0.650</td> <td align=center>0.652</td> <td align=center>0.546</td>
        </tr>
      </table>
    </div>
  </div>
</section>


<section class="section" id="bibtex">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">BibTeX</h2>
    <pre><code>@article{objcd,
  author    = {Doi, Kento and Hamaguchi, Ryuhei and Iwasawa, Yusuke and Onishi, Masaki and Matsuo, Yutaka and Sakurada, Ken},
  title     = {Detecting Object-Level Scene Changes in Images with Viewpoint Differences Using Graph Matching},
  journal   = {Remote Sensing},
  volume    = {14},
  number    = {17},
  year      = {2022},
}</code></pre>
  </div>
</section>


<!-- Acknowledgement -->
<section class="section" id="acknowledgement">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">Acknowledgment</h2>
    <p>
      This work was partially supported by JSPS KAKENHI (grantnumber:20H04217).
    </p>
  </div>
</section>


<!-- References -->
<section class="section" id="references">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">References</h2>
    <ol>
      <li> A. Varghese, J. Gubbi, A. Ramaswamy. (2018).
          "ChangeNet: A Deep Learning Architecture for Visual Change Detection.":
          In Proceedings of the ECCV Workshop.
      <li> K. Sakurada, M. Shibuya, W. Wang. (2020).
          "Weakly Supervised Silhouette-based Semantic Scene Change Detection.":
          In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 6861–6867.
    </ol>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://homes.cs.washington.edu/~kpar/nerfies/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This webpage is built with the template from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
            We sincerely thank <a href="https://keunhong.com/">Keunhong Park</a> for developing and open-sourcing this template.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
